# 🚀 Comment Toxicity Detection  

A deep learning-based model for detecting toxic online comments, using **Bidirectional LSTM** and **word embeddings** for text classification.  

![Interface](https://github.com/Muralidhar-77/Comment-Toxicity-Detection/blob/main/img.png)  

## 📌 Features  
✅ Multi-label classification for detecting **toxic, severe toxic, obscene, threat, insult, identity hate** categories.  
✅ Uses **LSTM for sequential text analysis** and **word embeddings for feature extraction**.  
✅ Achieved **97% accuracy** after fine-tuning the model.  

## 📂 Dataset  
- Dataset: 159,571 comments with corresponding labels.  
- Source: [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)  

## 🛠️ Technologies Used  
- **Python**  
- **TensorFlow / Keras**  
- **NLTK & SpaCy**  
- **Pandas & NumPy**  


## 📈 Results  
- Achieved **high accuracy and F1-score** on test data.  
- Optimized model for **better generalization and reduced false negatives**.  

## 📌 How to Run  
1. Clone the repository:  
   ```bash
   git clone https://github.com/yourusername/comment-toxicity-detection.git
