# ğŸš€ Comment Toxicity Detection  

A deep learning-based model for detecting toxic online comments, using **Bidirectional LSTM** and **word embeddings** for text classification.  

![Interface](https://github.com/Muralidhar-77/Comment-Toxicity-Detection/blob/main/img.png)  

## ğŸ“Œ Features  
âœ… Multi-label classification for detecting **toxic, severe toxic, obscene, threat, insult, identity hate** categories.  
âœ… Uses **LSTM for sequential text analysis** and **word embeddings for feature extraction**.  
âœ… Achieved **97% accuracy** after fine-tuning the model.  

## ğŸ“‚ Dataset  
- Dataset: 159,571 comments with corresponding labels.  
- Source: [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)  

## ğŸ› ï¸ Technologies Used  
- **Python**  
- **TensorFlow / Keras**  
- **NLTK & SpaCy**  
- **Pandas & NumPy**  


## ğŸ“ˆ Results  
- Achieved **high accuracy and F1-score** on test data.  
- Optimized model for **better generalization and reduced false negatives**.  

## ğŸ“Œ How to Run  
1. Clone the repository:  
   ```bash
   git clone https://github.com/yourusername/comment-toxicity-detection.git
